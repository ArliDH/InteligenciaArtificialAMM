# -*- coding: utf-8 -*-
"""Laboratorio N°1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WVsyu770eapozxX9sN9Q4IVi3t7TweLg
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

df1 = pd.read_csv("/content/amazon.csv")

def limpiar_precio(valor):
    return float(valor.replace("₹","").replace(",","").strip())

df1["discounted_price"] = df1["discounted_price"].apply(limpiar_precio)
df1["actual_price"] = df1["actual_price"].apply(limpiar_precio)

X = df1["discounted_price"].values
Y = df1["actual_price"].values

m = Y.size

# Normalización

X = X / max(X)
Y = Y / max(Y)

from matplotlib import pyplot

def plotData(x, y):
    #Grafica los puntos x e y en una figura nueva.

    fig = pyplot.figure()  # abre una nueva figura

    pyplot.plot(x, y, 'ro', ms=10, mec='k')
    pyplot.ylabel('Precio Actual')
    pyplot.xlabel('Descuento Actual')

plotData(X, Y)
pyplot.show()

X = np.stack([np.ones(m), X], axis=1)

def calcularCosto(X, y, theta):
    # inicializa algunos valores importantes
    m = y.size  # numero de ejemplos de entrenamiento

    J = 0
    # h = np.dot(X, theta)
    J = (1/(2 * m)) * np.sum(np.square(np.dot(X, theta) - y))
    return J

def gradientDescent(X, y, theta, alpha, num_iters):
        # Inicializa algunos valores importantes
    m = y.shape[0]  # numero de ejemplos de entrenamiento

    # hace una copia de theta, para evitar cambiar la matriz original,
    # ya que las matrices numpy se pasan por referencia a las funciones

    theta = theta.copy()

    J_history = [] # Lista que se utiliza para almacenar el costo en cada iteración

    for i in range(num_iters):
        theta = theta - (alpha / m) * (np.dot(X, theta) - y).dot(X)
        # save the cost J in every iteration
        J_history.append(calcularCosto(X, y, theta))

    return theta, J_history

# inicializa los parametros de ajuste
theta = np.zeros(2)

# configuraciones para el descenso por el gradiente
iterations = 500000
alpha = 0.001

theta, J_history = gradientDescent(X , Y, theta, alpha, iterations)
print('Theta encontrada por descenso gradiente: {:.4f}, {:.4f}'.format(*theta))
print(J_history)

# Grafica la convergencia del costo
pyplot.plot(np.arange(len(J_history)), J_history, lw=2)
pyplot.xlabel('Numero de iteraciones')
pyplot.ylabel('Costo J')

# Grafica la convergencia del costo
pyplot.plot(X, Y, 'ro', ms=10, mec='k')
pyplot.xlabel('PRECIO ACTUAL')
pyplot.ylabel('PRECIO DESCUENTO')
pyplot.plot(X[:, 1], np.dot(X, theta), '-')
pyplot.legend(['Puntos de datos', 'Regresión lineal']);